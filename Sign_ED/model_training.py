import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Conv1D, Masking, TimeDistributed, Flatten
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import numpy as np
import os
import pickle # ë‚˜ì¤‘ì— ì¶”ê°€ ã…œã…œ

# ì œìŠ¤ì²˜ ì •ì˜
actions = [
        'ì•ˆë…•,ì•ˆë¶€', 'ì‹¤ìˆ˜', 'ì‚´ë‹¤,ì‚¶,ìƒí™œ', 'ì·¨ë¯¸',
        'ì•„ë¹ ,ë¶€,ë¶€ì¹œ,ì•„ë¹„,ì•„ë²„ì§€', 'ê±´ê°•,ê¸°ë ¥,ê°•ê±´í•˜ë‹¤,íŠ¼íŠ¼í•˜ë‹¤', 'ì‰¬ë‹¤,íœ´ê°€,íœ´ê²Œ,íœ´ì‹,íœ´ì–‘',
        'ê³ ëª¨', 'ë‹¤ë‹ˆë‹¤', 'ì£½ë‹¤,ëŒì•„ê°€ë‹¤,ì‚¬ê±°,ì‚¬ë§,ì„œê±°,ìˆ¨ì§€ë‹¤,ì£½ìŒ,ì„ì¢…', 'ë‚¨í¸,ë°°ìš°ì,ì„œë°©',
        'ëª¸ì‚´', 'ê²°í˜¼,í˜¼ì¸,í™”í˜¼', 'ë…¸ë˜,ìŒì•…,ê°€ìš”', 'ë™ìƒ', 'ëª¨ì(ê´€ê³„)', 'ì‹ ê¸°ë¡'
    ]

# ë°ì´í„° ë¡œë“œ
x_train = np.load('./dataset/x_train.npy')
y_train = np.load('./dataset/y_train.npy')
x_val = np.load('./dataset/x_val.npy')
y_val = np.load('./dataset/y_val.npy')

# âœ… sequence_length ì ìš©
sequence_length = 15  # ê¸°ì¡´ 30 â†’ 15ë¡œ ë³€ê²½

# âœ… 30í”„ë ˆì„ ì¤‘ 15í”„ë ˆì„ì”© ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©
def sliding_window(data, seq_length):
    new_sequences = []
    for seq in data:
        for i in range(len(seq) - seq_length + 1):
            new_sequences.append(seq[i:i+seq_length])
    return np.array(new_sequences, dtype='float32')

# âœ… í•™ìŠµ ë°ì´í„° ë³€í™˜
x_train = sliding_window(x_train, sequence_length)
x_val = sliding_window(x_val, sequence_length)

# âœ… y_train, y_valë„ ìŠ¬ë¼ì´ë”© ì ìš© í›„ ë°ì´í„° ê°œìˆ˜ ë§ì¶”ê¸°
y_train = np.repeat(y_train, x_train.shape[0] // y_train.shape[0], axis=0)
y_val = np.repeat(y_val, x_val.shape[0] // y_val.shape[0], axis=0)

print(f"ğŸ” x_train.shape: {x_train.shape}, y_train.shape: {y_train.shape}")
print(f"ğŸ” x_val.shape: {x_val.shape}, y_val.shape: {y_val.shape}")


# F1-score ì»¤ìŠ¤í…€ ì§€í‘œ ì •ì˜
def metric_F1score(y_true, y_pred):
    TP = tf.reduce_sum(y_true * tf.round(y_pred))
    FP = tf.reduce_sum((1 - y_true) * tf.round(y_pred))
    FN = tf.reduce_sum(y_true * (1 - tf.round(y_pred)))

    precision = TP / (TP + FP + 1e-7)
    recall = TP / (TP + FN + 1e-7)
    F1score = 2 * precision * recall / (precision + recall + 1e-7)

    return F1score

# ëª¨ë¸ ì •ì˜
model = Sequential([
    # íŒ¨ë”©ëœ ë°ì´í„° ë¬´ì‹œ
    Masking(mask_value=0.0, input_shape=(x_train.shape[1], x_train.shape[2])),

    # Conv1D ì¶”ê°€ (Temporal CNN) â†’ ì‹œí€€ìŠ¤ì˜ ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ
    Conv1D(64, kernel_size=3, activation='relu', padding='same'),

    # Conv1D ì¶œë ¥ì´ 3D í˜•íƒœë¡œ ìœ ì§€ë˜ë„ë¡ TimeDistributed ì ìš©
    TimeDistributed(Flatten()),  # Flatten()ì„ ì‹œí€€ìŠ¤ ë‹¨ìœ„ë¡œ ì ìš©

    # Bidirectional LSTM â†’ ì‹œê°„ ì •ë³´ë¥¼ í•™ìŠµ
    Bidirectional(LSTM(64, return_sequences=False)),  # ìµœì¢… ì¶œë ¥ì€ (batch_size, 64)
    Dropout(0.3),

    # Fully Connected Layer
    Dense(32, activation='relu'),
    Dropout(0.3),

    # ìµœì¢… Softmax Layer
    Dense(len(actions), activation='softmax')
])

# ëª¨ë¸ ì»´íŒŒì¼ (F1-score ì¶”ê°€ë¨)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

# í•™ìŠµ ì½œë°± ì„¤ì • (`val_acc` ì‚¬ìš©)
early_stopping = EarlyStopping(monitor='val_acc', patience=20, mode='max')
callbacks = [
    ModelCheckpoint('./model/Sign_ED_best.keras', monitor='val_acc', save_best_only=True),
    ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=10),  # val_acc ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë¥  ê°ì†Œ
    early_stopping
]

# ëª¨ë¸ í•™ìŠµ
history = model.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    epochs=200,
    batch_size=32,
    callbacks=callbacks
)

# ëª¨ë¸ ì €ì¥ ( keras ë¡œ ë³€ê²½ )
model.save("./model/Sign_ED.keras")

# ëª¨ë¸ êµ¬ì¡° í™•ì¸
model.summary()

# ëª¨ë¸ ìš”ì•½ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥
with open("./model/model_summary.txt", "w") as f:
    model.summary(print_fn=lambda x: f.write(x + "\n"))

# í•™ìŠµ í›„ history ì €ì¥ - ë‚˜ì¤‘ì— ì¶”ê°€ ã…œã…œ
with open('./model/history.pkl', 'wb') as f:
    pickle.dump(history.history, f)
